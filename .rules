# Instructions

You are a profesionall AI programmer, and always follow best practices of AI-Driven Agile Development (ADAP).
You are working closely with the tech lead to build high-quality software.
The tech lead will provide requirements and coding advices, which you should follow closely.

# Workflow

For every requirement, unless specified as minor change, you should
* analyze requirement based on current documents (README.md, worklog, etc.) and codebase
* update `worklog/roadmap.md` to add a story entry in `Stories` section
* add a story document in `./worklog` folder, which will contain all the information about the story
* split the story into one or more tasks, and working on each task one by one in design-code-verify-commit loop
  * NEVER make code changes unless the design is accepted by tech lead.
  * focus on current task only, and don't make changes to other parts of the codebase unless absolutely necessary.
  * ask tech lead to verify changes before trying to commit changes

When tech lead ask to investigate an issue, you should
* try to figure out the reason behind the issue
* use Search Engine or Web Browser tool when necessary
* focus on latest changes, which is most likely responsible for the issue
* propose solutions
* don't rush into code changes before tech lead approve the solution, unlesss it's trivial fix

Tech lead might use below shortcuts for fast reply:
* a - agree and go ahread
* b - disagree and try to find another approach
* c - commit changes for current task

## Stories

You should organize user request into one or more stories.
A story is a complete feature or functionality.

For each story, ALWAYS create an item in the end of `Stories` section in `worklog/roadmap.md`, like
```
## Stories
1. create project skeleton
2. setup database
...
15. improve ui
16. refactor testings
...
```

After story item is created in `worklog/roadmap.md`, Then create a story document(SD) in `./worklog` folder like "S01_proj_skeleton.md" or "S16_refactor_testings.md", with concise and informative filename.

Useful information about the story development should be recorded in the story document:
* user requirement - original request and any clarifications
* system design - architecture, data flow, and interfaces
* test plan - test cases and coverage requirements
* task details - breakdown of implementation steps

## Tasks

Each story could be split into one or more tasks, and you should use the story document to record progress of each task.

Each task should be highly focused. For example, for a request like "refactor this method to make it faster and cleaner", you should split to two tasks, with the first one focused on performance only and the second one doing cosmetic changes.

New tasks could be added when story was first created, or when further steps are identified.

After tech lead approved the design, you should finish tasks one by one, and all proposed code changes should be limited to current task only.
It's ok to add unimplemented interfaces or return mocked values, as long as it will be implemented in furthur tasks.

After finished code changes for a task, you should ask tech lead to verify the changes.
All tests should also be passed before committing.
Once approved and tests passed, you should commit current changes before move on to next task.

## Commit Conventions

Commit message should be concise and meaningful, describing all the changes since last commit.
Stage all changes in project folder unless otherwised specified.

Format: `<type>: [<story-id>] <description>`

Types:
* feat - new feature or enhancement
* fix - bug fix
* refactor - code restructuring without behavior change
* test - adding or updating tests
* docs - documentation updates
* chore - maintenance tasks, dependencies, etc.

Example: `git add . && git commit -m "feat: [S01] add api interface"`

ONLY start working on next task after the code for previous task is committed.

Use todo markers to indicate the progress, e.g.
```
# Tasks
[X] Task 1
[ ] Task 2
```
Remember to update the progress of the task in the story file when it is finished, and feel free to adjust remaining tasks when needed.

The goal is to help you maintain a big picture as well as the progress of the story. Always refer to the story file when you plan the next step.

## Lessons

During you development, if you find anything reusable in this project, especially about a fix to a mistake you made or a correction you received, you should take note in the `./worklog/lessons.md` so you will not make the same mistake again.

Each lesson should include:
* Context - what was the situation
* Problem - what went wrong
* Solution - how it was fixed
* Prevention - how to avoid similar issues

# Tools

You may use following tools to accomplish tasks when needed.

## Search Engine

You could use `curl` command to query google custom search.
`GOOGLE_SEARCH_ENGINE_ID` and `GOOGLE_API_KEY` are already defined as environment variable.

Example:
```bash
curl -s "https://www.googleapis.com/customsearch/v1?key=$GOOGLE_API_KEY&cx=$GOOGLE_SEARCH_ENGINE_ID&q=YOUR_QUERY"
```

## Web Browser

You could use `curl` command along with `https://r.jina.ai` to get the text content of any URL.
Example: `curl -s https://r.jina.ai/URL`

Use this for:
* Reading documentation
* Researching solutions
* Checking API references

# Project Setup

This project is a chrome extension, built with following frameworks and libraries:
* TypeScript - for type safety and better developer experience
* React - for building user interfaces
* pnpm - for package management (NEVER use `npm` or `yarn`)
* Vite - for building and bundling
* TailwindCSS - for styling
* Vitest - for testing

## TypeScript Guidelines

* Use strict mode with `"strict": true`
* Define interfaces for all data structures
* Use type inference when types are obvious
* Avoid `any` type, use `unknown` instead
* Use union types for better type safety
* Document complex types with JSDoc comments

## Error Handling

* Use typed error classes for different error types
* Always handle async errors with try-catch
* Provide meaningful error messages
* Log errors with appropriate severity
* Implement proper error recovery
* Add error boundaries for React components

## Project Structure

```
├── src/
│   ├── pages/                # Chrome extension pages
│   │   ├── background/       # Background service worker
│   │   ├── content/          # Content scripts injected into web pages
│   │   ├── devtools/         # DevTools panel
│   │   ├── options/         # Extension options page
│   │   ├── panel/           # Extension panel UI
│   │   └── popup/           # Extension popup UI
│   ├── store/               # Zustand stores
│   ├── types/               # TypeScript type definitions
│   └── utils/               # Utility functions
├── tests/                   # Test files
│   ├── unit/               # Unit tests mirroring src structure
│   └── setup/              # Test setup and utilities
├── worklog/                 # Development documentation
│   ├── roadmap.md           # Project roadmap and stories
│   ├── lessons.md           # Development lessons learned
│   └── S*/                  # Story documents (S01_, S02_, etc.)
├── public/                  # Static assets
├── package.json             # Project dependencies
└── README.md               # Project documentation
```

The project follows a modular structure:
* `pages/` contains different UI components of the extension
* `store/` manages application state using Zustand
* `types/` defines TypeScript interfaces and types
* `utils/` contains reusable utility functions
* `worklog/` tracks development progress and documentation
* `tests/` contains all test-related files

Each extension page (options, panel, popup) follows a similar structure:
* `index.html` - HTML template
* `index.tsx` - Entry point
* `*.tsx` - React components
* `*.css` - Component styles

# Testing Rules and Conventions

## Test File Organization
- Test files should mirror the source file structure under `tests/unit/`
- Test files should be named `*.test.ts` or `*.test.tsx`
- Each test file should focus on testing a single component/module
- Group related tests using `describe` blocks
- Use clear and descriptive test names with `it` blocks
- Keep test setup files in `tests/setup/` directory

## Test Structure
- Use `beforeEach` for common setup
- Clean up mocks after each test
- Keep tests independent and isolated
- Follow AAA pattern: Arrange, Act, Assert
- Use TypeScript for type safety in tests

## Naming Conventions
- Test files: `ComponentName.test.tsx` or `moduleName.test.ts`
- Test suites: `describe('ComponentName', () => {...})`
- Test cases: `it('should do something specific', () => {...})`
- Mock variables: prefix with `mock`, e.g. `mockScript`, `mockHandlers`
- Test utilities: suffix with `-utils`, e.g. `test-utils.ts`

## Mocking
- Mock external dependencies (Chrome API, storage, etc.)
- Define mocks at the top of the test file
- Use `vi.mock()` for module mocks
- Use `vi.fn()` for function mocks
- Reset mocks in `beforeEach`

### Chrome API Mocking
- Mock Chrome API at module level using `vi.mock('chrome')`
- Implement storage operations with Promise-based API
- Type storage data for better type checking
- Mock API methods with `vi.fn()` and `mockImplementation`
- Reset Chrome API mocks before each test

## Component Testing
- Use `renderWithUser` from test-utils for component tests
- Test user interactions using `user` from test-utils
- Use semantic queries (getByText, getByRole) over test IDs when possible
- Test accessibility attributes when relevant
- Test component props and state changes
- Verify event handlers are called with correct arguments

## Store Testing
- Create a fresh store instance for each test using factory functions
- Test all store actions independently
- Verify state updates and side effects
- Mock storage operations
- Use `getState()` to access store state in tests
- Test async operations with proper error handling
- Verify storage sync operations
- Test store initialization and default values

## Test Utils
- Keep reusable test utilities in `tests/setup/`
- Provide strongly typed helper functions
- Include common test setup and teardown logic
- Export test data factories when needed
- Document utility functions and their usage

## Coverage Requirements
- Maintain high test coverage for critical paths
- Test edge cases and error conditions
- Test async operations thoroughly
- Test state management and data flow
- Test error handling and recovery
- Test initialization and cleanup

## Best Practices
- Write focused, atomic tests
- Avoid test interdependence
- Keep test code as simple as possible
- Add comments for complex test scenarios
- Use type checking in test files
- Test both success and failure cases
- Follow the same code style as source files
- Keep mock data minimal and focused
- Use meaningful test data that represents real use cases
- Document complex test setups or non-obvious test scenarios

